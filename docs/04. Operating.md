# Overview

Up until now we have operated in development mode, with only a single Jet instance which is colocated with the code that builds the pipeline.  In this lab we will see how to package a job and deploy it to a pre-existing Jet cluster.   We'll also see how to update a job without missing any events.

We will not add any pipeline stages in this lab. We will just be separating the Jet cluster from the Jet job definition.  We will also introduce a UI that will allow us to visualize the GPS pings.

The resulting system will look like this:

![schematic 4](media/schematic_4.png)



Throughout this lab you will want to make frequent reference to [section 3](https://docs.hazelcast.org/docs/jet/3.2/manual/#work-with-jet) of the Jet Reference Manual.

Expected Time: 1 hr 40 min.

# Part 1 - Configure Enterprise Features and Logging

__Warning: __ if your development machine is close to the required minimum specifications this lab may overload it.  In order to minimize the possibility, it is strongly recommended to reboot your machine and run only what is necessary.



In this  part, we will bring up a 2 node Jet cluster as well as all of the other parts of the system and then we will deploy our job to the Jet cluster using the `jet.sh` script.  First , we will configure logging, fault tolerance and hot restart for Jet.

Jet logging is controlled by the underlying IMDG instance. Jet and IMDG can be configured to use any of the  popular logging frameworks.   For details see the [logging configuration section](https://docs.hazelcast.org/docs/3.12.4/manual/html-single/index.html#logging-configuration) of the IMDG manual.  

Soon we will be making an update to the ingest pipeline and we want to observe how Jet behaves during the update. 

1. Update  `BetaStreamSource` with logging as as shown below.  This will be used later to observe how Jet handles restarts.

```java
    public void fillBuffer(SourceBuilder.TimestampedSourceBuffer<Ping> buffer){
        Ping[] pings = poll(highestSequence, 200);

        for(Ping p: pings){
            buffer.add(p, (long) (p.getTime() * 1000.0));
        }

      	// add this to log the highest sequence number
        if (pings.length > 0) {
            highestSequence = pings[pings.length - 1].getSequence();
            logger.fine("Added " + pings.length + " pings. Highest sequence number is: " + highestSequence);
        }

    }

```



2. Copy `lab04.yml` to `docker-compose.yml`

3. Configure Logging	
   1. If the system is still running, stop it using `docker-compose down`

   2. Add the dependency for your selected logging framework to the `jet-server` project (not the pipeline).

   ```XML
   <dependency>
       <groupId>org.apache.logging.log4j</groupId>
       <artifactId>log4j-core</artifactId>
       <version>2.12.1</version>
   </dependency>
   ```

   *You will need to repackage the project so that the new dependency will be included.*

   `mvn package`

4. Configure IMDG to use log4j2 by editing `docker-compose.yml` to pass the correct -D options as shown below.  Make sure to make this change for all Jet server instances in the file.

   ```yml
   jet-server-1:
    container_name: jet-server-1
    image: hazelcast/hazelcast-jet-enterprise@sha256:95138f113992b8a98865cc42af59b918b1a9edff4f2d70850f69d8e29996b5cb
    volumes:
      - ".:/opt/project"
    command: >
      java
      -cp /opt/project/jet-server/target/jet-server-1.0-SNAPSHOT.jar
      -Dhazelcast.config=/opt/project/config/hazelcast.xml
      -Dhazelcast.enterprise.license.key=${JET_LICENSE_KEY}
      -Dhazelcast.logging.type=log4j2
      -Dlog4j2.configurationFile=/opt/project/config/log4j2.xml
      com.hazelcast.training.streams.server.Server
   ```

5. Review `config/log4j2.xml`.  Set the logging level to "trace" for "com.hazelcast.training" `BetaStreamSource.java`.  

6. Now, configure hot restart persistence. This configuration must be done in 2 places: the Jet configuration file and the IMDG configuration file.  Note that in `docker-compose.yml` we are using "-Dhazelcast.jet.config" to explicitly point Jet to the configuration file at `/opt/project/config/hazelcast-jet.xml`.

   Edit `hazelcast-jet.xml` to enable lossless restart.  	

```xml
<hazelcast-jet xsi:schemaLocation="http://www.hazelcast.com/schema/jet-config hazelcast-jet-config-3.0.xsd"
               xmlns="http://www.hazelcast.com/schema/jet-config"
               xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance">
    <instance>
        <lossless-restart-enabled>true</lossless-restart-enabled>
    </instance>
</hazelcast-jet>
```



Then, in the IMDG configuration file, `hazelcast.xml`, configure the location of the hot restart disk stores as shown below.

```xml
...
    <hot-restart-persistence enabled="true">
     <base-dir>/opt/project/host-restart</base-dir>
     <auto-remove-stale-data>true</auto-remove-stale-data>
    </hot-restart-persistence>
...
```

7. Start the environment with `docker-compose up -d`.

This will start the following things:

- A 2 node Jet cluster
- IMDG Management Center at http://localhost:8080/hazelcast-mancenter
- Jet Management Center at http://localhost:8081
- The "fleetsim-alpha" data generator which writes CSV records to `/opt/project/data/alpha`
- The "fleetsim-beta" web service at http://localhost:8000/pings
- The UI, which can be accessed at http://localhost:5006/dashboard

Take a minute to verify everything is working.  

Currently, the UI should look like the one below, showing a map of the central US region with no icons on it.

<img src="media/blank_map.png" alt="blank map" style="zoom:50%;" />

  # Part 2 - Package the Job and Deploy It

Jobs that are deployed to a standalone cluster have a few small differences.  This section describes how to set up such a job and deploy it.

   1. Configure the job for fault tolerance. _This must be done per job in the code_.  Edit the "main" method of `ingest-pipeline/src/main/java/com/hazelcast/training/`You need to create a "JobConfig" instance and set the processing guarantee and the snapshot frequency.  See the sample code below.

      ```java
      ...
      JobConfig config = new JobConfig();
      config.setProcessingGuarantee(ProcessingGuarantee.AT_LEAST_ONCE);
      config.setSnapshotIntervalMillis(10 * 1000);
      
      String dir = args[0];
      String url = args[1];
      Pipeline pipeline = buildPipeline(dir, url);
      jet.newJob(pipeline, config);
      ```

   2. Package the job for deployment. 

      This is mostly done for you. Review `ingest-pipeling/pom.xml`. Note the following:

      - The shade plugin has been used to package all of the dependencies (e.g. gson) into a single jar.
      - A manifest entry has been added to indicate the main class.

      In `ingest-pipeline/pom.xml` change the scope of the "hazelcast-jet-enterprise" dependency to "provided".  There is no need to package these into the job because it will be running on a Jet node that already the Jet classes.

      ```xml
           <dependency>
               <groupId>com.hazelcast.jet</groupId>
               <artifactId>hazelcast-jet-enterprise</artifactId>
               <scope>provided</scope>
           </dependency>
      ```

   3. Package the jar.

      ```bash
      $ mvn package
      ```

      

   4. Deploy the job! You will find a new "admin-server" service in `docker-compose.yml`.  This is just a server with all of the Jet software on it that can be used to deploy your jet job.  In a real life situation this could be any server with the jet software installed and network connectivity to the jet cluster.

      ```bash
$ docker-compose run admin-server bash 
      # within the container now 
      
      $ jet.sh -f /opt/project/config/hazelcast-client.xml -v submit /opt/project/ingest-pipeline/target/ingest-pipeline-1.0-SNAPSHOT.jar /opt/project/data/alpha http://fleetsim-beta:8000/pings
      ```
      
      If all goes well, you should start to see entries in the "vehicles" map within IMDG management center and Jet management center.  Refresh the UI page.  Each ping should now be depicted on the UI (if you watch long enough you should be able to see the vehicles moving).

   <img src="media/map_with_pings.png" alt="map with pings" style="zoom:50%;" />

   5. As a bonus, pick a VIN from the data and display the corresponding map entry using the "Map Browser" feature of IMDG management center.

      ![map browser](media/map_browser.png)



6.  Use `docker-compose logs --follow` to view the logs and verify that you can see the logging output from the jet job.  An example is below.

   ```
fleetsim-beta             | INFO:werkzeug:172.24.0.6 - - [09/Dec/2019 16:01:12] "GET /pings?since=599&limit=200 HTTP/1.1" 200 -
jet-server-2              | 16:01:12.196 [hz._hzInstance_1_dev.jet.blocking.thread-0] DEBUG com.hazelcast.training.streams.ingest.BetaStreamSource - Added 3 pings. Highest sequence number is: 602
fleetsim-beta             | INFO:werkzeug:172.24.0.6 - - [09/Dec/2019 16:01:14] "GET /pings?since=602&limit=200 HTTP/1.1" 200 -
jet-server-2              | 16:01:14.196 [hz._hzInstance_1_dev.jet.blocking.thread-0] DEBUG com.hazelcast.training.streams.ingest.BetaStreamSource - Added 3 pings. Highest sequence number is: 605
fleetsim-beta             | INFO:werkzeug:172.24.0.6 - - [09/Dec/2019 16:01:16] "GET /pings?since=605&limit=200 HTTP/1.1" 200 -
fleetsim-beta             | INFO:werkzeug:172.24.0.6 - - [09/Dec/2019 16:01:18] "GET /pings?since=605&limit=200 HTTP/1.1" 200 -
jet-server-2              | 16:01:18.198 [hz._hzInstance_1_dev.jet.blocking.thread-0] DEBUG com.hazelcast.training.streams.ingest.BetaStreamSource - Added 1 pings. Highest sequence number is: 606

   ```

_Note: Leave the system running for the next lab._

# Part 3: Test Fault Tolerance, Scale Up



By default, when you add nodes to a cluster, any job with a configured processing guarantee will spread out to use all available members.  Try it now.  _If your environment allows_, add a 3rd jet server by modifying `docker-compose.yml`.  Be sure to change the container name to "jet-server-3" and the IP address to "192.168.1.103".  Run `docker-compose up -d` to start the 3rd container. After about 30 seconds you should see the job restart.  In the Jet Management Center, if you select the job, you will be able to see the number of nodes in use go from 2 to 3. That is how you scale up a Jet job!

Now we will see what happens when a member of the cluster fails.  Using the logs, determine which server is running the "Beta" source.  This will be the one that produces log messages like those below.

```
jet-server-2              | 19:43:02.825 [hz._hzInstance_1_dev.jet.blocking.thread-0] DEBUG com.hazelcast.training.streams.ingest.BetaStreamSource - Added 3 pings. Highest sequence number is: 1037
```

Now, simulate a failure of that server as follows.  _Be sure to use the correct container name, yours may not be jet-server-2_ . `docker-compose kill jet-server-2`.

After 20-30 seconds, you should see that the job is restarted and the "Beta" source will begin processing events where it left off .  

Remove jet-server-3 from docker-compose.yml.

# Part 4: Update a Deployed Job Without Missing Events - Case 1: No Change to Classes on the Server Classpath

You may have noticed that the classes defined in the ingest pipeline did not have to be installed on the server class path and no servers had to be restarted.  It just worked!  In this lab, we will update and redeploy a job.  

There are 2 cases. If all of the changes involve classes in the job that are not on the server's startup class path then making a change is a simple matter of cancelling and re-deploying the job.  This part of the lab falls into the easy case.  The next part will exercise the other case.



> Now is a good time to review the caveats about restartability of a Jet job.  The sources must be "rewindable",  the pipeline itself must produce deterministic results, and the sinks must be idempotent.  
>
> In our case, the sinks are definitely idempotent because any number of "map.puts" with the same key and value results in the same state as a single put.  The processing is also deterministic.  There are, for example, no external lookups on live data.  
>
> Now consider the sources. Beta is rewindable.  When we update the job, the Beta source will be  replayed from the last snapshot so we will not miss any events.  Alpha is not rewindable.  This means that, while the job is being updated,  events from Alpha will be missed.  



1. Now, use Jet Management to take a snapshot of the running job and cancel it.  To do this, select the job and press the "export snapshot" button.  Use the "Cancel job and export" option.

2. Now make a change to the pipeline.  Right before the pings are saved to the "vehicles" map add a line similar to the one below to log each one.

   ```java
     mapEntries.peek()
       .drainTo(Sinks.logger(t2 -> String.format("SAVING PING: %s", t2.f1())));
   ```

3. Now we just need to deploy the new job.  The command is the same except that we add the "-s"  option to tell Jet to start the job using the snapshot we just created.  The example below uses the snapshot name "my-snapshot".  Be sure to change that to the name you used when you created the snapshot.

   ```bash
   $ docker-compose  run admin-server bash
   # inside new container
   $ jet.sh -f /opt/project/config/hazelcast-client.xml -v submit -s my-snapshot /opt/project/ingest-pipeline/target/ingest-pipeline-1.0-SNAPSHOT.jar /opt/project/data/alpha http://fleetsim-beta:8000/pings
   ```

4. You should now be able to verify using the logs that the job resumed and immediately ingested all of the events that occurred while the job was down.

   ```
   fleetsim-beta             | INFO:werkzeug:172.28.0.6 - - [09/Dec/2019 21:24:52] "GET /pings?since=419&limit=200 HTTP/1.1" 200 -
   jet-server-1              | 21:24:52.274 [hz._hzInstance_1_dev.jet.blocking.thread-3] DEBUG com.hazelcast.training.streams.ingest.BetaStreamSource - Added 163 pings. Highest sequence number is: 769
   ```

5. This has created a lot of noise in the logs.  Finish this section by removing the debugging statement and re-deploying the job.

# 

# Part 5: Update a Deployed Job Without Missing Events - Case 2: Server Class Path Has Changed

Some classes must be deployed to the server class path.  This includes classes that are shared between jobs (e.g. domain classes) and Hazelcast IMDG server side code like EntryProcessors and Aggregators.  In these cases the easiest thing to do is simply to rely on the hot restart feature.  

The procedure is as follows: 

- stop the whole cluster
- update the classes
- start the cluster.  

When the system is started after a complete shutdown it will restore itself from the most recent snapshot and resume processing.

In this part, we will update the Ping class, which is on the server's class path.  Until now, we have only worked with the GPS information in our data sources but they also contain engine diagnostic codes which will be useful for multiple purposed including detecting when a crash has occurred and predicting required maintenance.

1. First, use the map browser feature to look at a few VINs and verify that the data does not already contain engine diagnostic codes.  The easiest way to get a VIN is from the web service.  The URL is localhost:8000/pings?since=1234.

2. Stop the cluster using the "shutdown" button in the Jet Management Center.  This performs a clean and orderly shutdown.  _Do not use docker to stop the cluster!_.  Verify that the jet servers have exited cleanly, waiting if necessary.

   ```
   MacBook-Pro:in-depth-streams-with-hazelcast randy$ docker-compose ps
            Name                       Command               State                      
   ----------------------------------------------------------------------------------------
   admin-server             echo EXITING                     Exit 0
   fleetsim-alpha           python fleetsim_alpha.py         Up
   fleetsim-beta            flask run -h 0.0.0.0 -p 8000     Up       0.0.0.0:8000->8000/tcp
   imdg-management-center   bash /mc-start.sh                Up       0.0.0.0:8080->8080/tcp, 8081/tcp, 8443/tcp
   jet-management-center    bash -c set -euo pipefail  ...   Up       0.0.0.0:8081->8081/tcp
   jet-server-1             java -Xmx128m -cp /opt/pro ...   Exit 0
   jet-server-2             java -Xmx128m -cp /opt/pro ...   Exit 0
   mapserver                bokeh serve /opt/project/U ...   Up       0.0.0.0:5006->5006/tcp
   Mac
   ```

   

3. Now  update the code. For this change, all you need to do is add the "obd_codes" field to the Ping java object at `common/src/main/java/com/hazelcast/training/streams/model/Ping.java`.   Once you've updated the Ping object, rebuilt the ingest-pipeline jar.  Note: since the ingest-pipeline project references the common project where Ping is, you will need to rebuild that as well.  The best way to do that is to run "mvn clean install" at the top level of the project.

   ```bash
   $ mvn clean install
   ```

4. Now just start the cluster members again.

   ```
   docker-compose up -d
   ```

   

5. The job should automatically resume.  Use the map browser in IMDG Management Center to verify that the new entries do indeed have obd codes.

   

# What We Learned

- How to package a Jet job for deployment to a running cluster.
- How to configure Jet logging and lossless restart.
- How to scale up a Jet cluster
- How Jet handles node failures
- 2 ways to update a Jet job